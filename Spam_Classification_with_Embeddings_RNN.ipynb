{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfdtoctZi2B/ZMUTZBjwOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavankuamr14/deeplearning/blob/main/Spam_Classification_with_Embeddings_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inTGugQymZMV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Spam Data"
      ],
      "metadata": {
        "id": "QFhFb8QSmjpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "#Load Spam Data and review content\n",
        "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
        "\n",
        "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
        "print(spam_data.head())\n",
        "\n",
        "#Separate feature and target data\n",
        "spam_classes_raw = spam_data[\"CLASS\"]\n",
        "spam_messages = spam_data[\"SMS\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHBAX8mRmktB",
        "outputId": "fb12eae9-8b35-46ef-aad5-9c2d834f21d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded Data :\n",
            "------------------------------------\n",
            "  CLASS                                                SMS\n",
            "0   ham   said kiss, kiss, i can't do the sound effects...\n",
            "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
            "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
            "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
            "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Spam Data"
      ],
      "metadata": {
        "id": "4ejyyPG8m4Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a label encoder for target variable to convert strings to numeric values.\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "spam_classes = label_encoder.fit_transform(\n",
        "                                spam_classes_raw)\n",
        "\n",
        "#Convert target to one-hot encoding vector\n",
        "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
        "\n",
        "print(\"One-hot Encoding Shape : \", spam_classes.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeKAnyBzmwEU",
        "outputId": "0f7dcf9a-1339-4cdc-875a-0e53725f7298"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot Encoding Shape :  (1500, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess data for spam messages\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Max words in the vocabulary for this dataset\n",
        "VOCAB_WORDS=10000\n",
        "#Max sequence length for word sequences\n",
        "MAX_SEQUENCE_LENGTH=100\n",
        "\n",
        "#Create a vocabulary with unique words and IDs\n",
        "spam_tokenizer = Tokenizer(num_words=VOCAB_WORDS)\n",
        "spam_tokenizer.fit_on_texts(spam_messages)\n",
        "\n",
        "\n",
        "print(\"Total unique tokens found: \", len(spam_tokenizer.word_index))\n",
        "print(\"Example token ID for word \\\"me\\\" :\", spam_tokenizer.word_index.get(\"me\"))\n",
        "\n",
        "#Convert sentences to token-ID sequences\n",
        "spam_sequences = spam_tokenizer.texts_to_sequences(spam_messages)\n",
        "\n",
        "#Pad all sequences to fixed length\n",
        "spam_padded = pad_sequences(spam_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"\\nTotal sequences found : \", len(spam_padded))\n",
        "print(\"Example Sequence for sentence : \", spam_messages[0] )\n",
        "print(spam_padded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pZf_9ybm7mt",
        "outputId": "b39ee186-20b6-482f-96d7-b9d3f56497fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique tokens found:  4688\n",
            "Example token ID for word \"me\" : 25\n",
            "\n",
            "Total sequences found :  1500\n",
            "Example Sequence for sentence :   said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0  260  921  921    4  430   55    6 1488 2294  148   10\n",
            "    3 1489  464 1143  148  922   19  514   77 1144    3  515    1 2295\n",
            "  397   89]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split into training and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "                                    spam_padded,spam_classes,test_size=0.2)"
      ],
      "metadata": {
        "id": "V_n_wZdpnH_D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the embeddding matrix"
      ],
      "metadata": {
        "id": "e3O-4TIEnoRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the pre-trained embeddings\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#Read pretrained embeddings into a dictionary\n",
        "glove_dict = {}\n",
        "\n",
        "#Loading a 50 feature (dimension) embedding with 6 billion words\n",
        "with open('glove.6B.50d.txt', \"r\", encoding=\"utf8\") as glove_file:\n",
        "    for line in glove_file:\n",
        "\n",
        "        emb_line = line.split()\n",
        "        emb_token = emb_line[0]\n",
        "        emb_vector = np.array(emb_line[1:], dtype=np.float32)\n",
        "\n",
        "        if emb_vector.shape[0] == 50:\n",
        "            glove_dict[emb_token] = emb_vector\n",
        "\n",
        "print(\"Dictionary Size: \", len(glove_dict))\n",
        "print(\"\\n Sample Dictionary Entry for word \\\"the\\\" :\\n\", glove_dict.get(\"the\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG_VLBqdnlug",
        "outputId": "72a41ee0-42d2-4d23-bedc-3370477ae0c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary Size:  14712\n",
            "\n",
            " Sample Dictionary Entry for word \"the\" :\n",
            " [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We now associate each token ID in our data set vocabulary to the corresponding embedding in Glove\n",
        "#If the word is not available, then embedding will be all zeros.\n",
        "\n",
        "#Matrix with 1 row for each word in the data set vocubulary and 50 features\n",
        "\n",
        "vocab_len = len(spam_tokenizer.word_index) + 1\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_len, 50))\n",
        "\n",
        "for word, id in spam_tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = glove_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[id] = embedding_vector\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"Size of Embedding matrix :\", embedding_matrix.shape)\n",
        "print(\"Embedding Vector for word \\\"me\\\" : \\n\", embedding_matrix[spam_tokenizer.word_index.get(\"me\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk6fhlxknsLB",
        "outputId": "b0b0ab9c-c257-4554-8d80-465732808c20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Embedding matrix : (4689, 50)\n",
            "Embedding Vector for word \"me\" : \n",
            " [-0.14524999  0.31265     0.15184    -0.63708001  0.63552999 -0.50295001\n",
            " -0.23214     0.52891999 -0.58629     0.53934997 -0.3055      1.03569996\n",
            " -0.77989    -0.19386999  1.22150004  0.24521001  0.26144001  0.22439\n",
            "  0.15583999 -0.79145998 -0.65262002  1.3211      0.76617998  0.38234001\n",
            "  1.44529998 -2.26430011 -1.15050006  0.50373     1.2651     -1.59029996\n",
            "  3.05180001  0.84118003 -0.69542998  0.29984999 -0.49151    -0.22312\n",
            "  0.59527999 -0.076347    0.52358001 -0.50133997  0.22483     0.01546\n",
            " -0.088005    0.21281999  0.28545001 -0.15976    -0.16777    -0.50895\n",
            "  0.14322001  1.01180005]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0X2rKDQiomFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Spam Model with Embeddings"
      ],
      "metadata": {
        "id": "2qypjMSlopkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.layers import LSTM,Dense\n",
        "\n",
        "#Setup Hyper Parameters for building the model\n",
        "NB_CLASSES=2\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Embedding(vocab_len,\n",
        "                                 50,\n",
        "                                 name=\"Embedding-Layer\",\n",
        "                                 weights=[embedding_matrix],\n",
        "                                 input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                 trainable=True))\n",
        "\n",
        "#Add LSTM Layer\n",
        "model.add(LSTM(256))\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "                             name='Output-Layer',\n",
        "                             activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcX0H0-QoqeS",
        "outputId": "e2feb037-a303-4708-fefa-2981f536b241"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding-Layer (Embedding  (None, 100, 50)           234450    \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               314368    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " Output-Layer (Dense)        (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 549332 (2.10 MB)\n",
            "Trainable params: 549332 (2.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make it verbose so we can see the progress\n",
        "VERBOSE=1\n",
        "\n",
        "#Setup Hyper Parameters for training\n",
        "BATCH_SIZE=256\n",
        "EPOCHS=10\n",
        "VALIDATION_SPLIT=0.2\n",
        "\n",
        "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
        "\n",
        "history=model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=VERBOSE,\n",
        "          validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
        "model.evaluate(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD1yDAbHotlq",
        "outputId": "0584d83e-6651-451b-fb7f-43a258786ad9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Progress:\n",
            "------------------------------------\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 10s 2s/step - loss: 0.6478 - accuracy: 0.6896 - val_loss: 0.4348 - val_accuracy: 0.8583\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.3428 - accuracy: 0.9000 - val_loss: 0.2628 - val_accuracy: 0.9000\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.2920 - accuracy: 0.8698 - val_loss: 0.2822 - val_accuracy: 0.8917\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.2206 - accuracy: 0.9187 - val_loss: 0.2305 - val_accuracy: 0.9167\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 7s 2s/step - loss: 0.2005 - accuracy: 0.9281 - val_loss: 0.3437 - val_accuracy: 0.8458\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.2225 - accuracy: 0.9156 - val_loss: 0.2123 - val_accuracy: 0.9208\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.1786 - accuracy: 0.9271 - val_loss: 0.2108 - val_accuracy: 0.9208\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.1868 - accuracy: 0.9375 - val_loss: 0.1871 - val_accuracy: 0.9458\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.1360 - accuracy: 0.9542 - val_loss: 0.1856 - val_accuracy: 0.9375\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1765 - accuracy: 0.9333 - val_loss: 0.2125 - val_accuracy: 0.9250\n",
            "\n",
            "Evaluation against Test Dataset :\n",
            "------------------------------------\n",
            "10/10 [==============================] - 1s 80ms/step - loss: 0.1828 - accuracy: 0.9467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18283796310424805, 0.9466666579246521]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ikdQTLixo0Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Spam"
      ],
      "metadata": {
        "id": "XAhg6Y0ro29O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two input strings to predict\n",
        "input_str=[\"Unsubscribe send GET EURO STOP to 83222\",\n",
        "            \"Yup I will come over\"]\n",
        "\n",
        "#Convert to sequence using the same tokenizer as training\n",
        "input_seq = spam_tokenizer.texts_to_sequences(input_str)\n",
        "#Pad the input\n",
        "input_padded = pad_sequences(input_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "#Predict using model\n",
        "prediction=np.argmax( model.predict(input_padded), axis=1 )\n",
        "print(\"Prediction Output:\" , prediction)\n",
        "\n",
        "#Print prediction classes\n",
        "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGSbsEJeo314",
        "outputId": "04a4a794-fab3-4aa2-c36e-6e5c4d303f18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 479ms/step\n",
            "Prediction Output: [1 0]\n",
            "Prediction Classes are  ['spam' 'ham']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DlHXFucPpFDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}